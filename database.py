import aiohttp
import json
import logging
import asyncio
from datetime import datetime, timedelta

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ SUPABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUPABASE_URL = "https://tuvqserdclbgloysblrx.supabase.co"
SUPABASE_KEY = "sb_secret_bDIUtmYZ2Zx5Rz3EauEhlw_sbrmR6y9" # Ğ¢Ğ²Ğ¾Ğ¹ ÑĞµĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡

# Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=minimal" # ĞĞµ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ Ñ‚Ñ€Ğ°Ñ„Ğ¸Ğº)
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ‘ĞĞ—Ğ« Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def create_pool():
    """
    Ğ”Ğ»Ñ ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ main.py. 
    Ğ’ aiohttp Ğ¿ÑƒĞ»Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ¸Ğ½Ğ°Ñ‡Ğµ, Ğ½Ğ¾ Ğ¼Ñ‹ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ.
    """
    logging.warning("âœ… Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ HTTP ÑĞµÑÑĞ¸Ğ¸ Ğ´Ğ»Ñ Supabase...")
    # ĞœĞ¾Ğ¶Ğ½Ğ¾ ÑĞ´ĞµĞ»Ğ°Ñ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²ÑÑ‘ Ğ¾Ğº
    async with aiohttp.ClientSession() as session:
        url = f"{SUPABASE_URL}/rest/v1/"
        async with session.get(url, headers=HEADERS) as resp:
            if resp.status == 200:
                logging.warning("âœ… Ğ¡Ğ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Supabase ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¾!")
            else:
                logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ñ Supabase: {resp.status}")

async def create_table():
    """
    Ğ’ REST API Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ»ÑƒÑ‡ÑˆĞµ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Supabase (SQL Editor).
    ĞÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ main.py Ğ½Ğµ Ğ»Ğ¾Ğ¼Ğ°Ğ»ÑÑ Ğ¿Ñ€Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğµ.
    """
    pass

async def save_all_users(users_dict):
    """
    Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ‡ĞµÑ€ĞµĞ· HTTP Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ (UPSERT).
    """
    if not users_dict:
        return

    today = datetime.now().strftime("%Y-%m-%d")
    data_list = []

    # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
    for user_id, data in users_dict.items():
        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ JSON Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ñ json_data
        # Ğ’Ğ°Ğ¶Ğ½Ğ¾: Supabase Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ json Ğ±Ñ‹Ğ» Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ¼ Ğ¸Ğ»Ğ¸ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹, 
        # aiohttp ÑĞ°Ğ¼ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ dict Ğ² json Ğ¿Ñ€Ğ¸ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞµ, Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ñ jsonb 
        # Ğ»ÑƒÑ‡ÑˆĞµ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ ĞºĞ°Ğº ĞµÑÑ‚ÑŒ, Supabase Ğ¿Ğ¾Ğ¹Ğ¼ĞµÑ‚.
        
        row = {
            "user_id": user_id,
            "username": data.get('username', 'Guest'),
            "nickname": data.get('nickname', 'Unknown'),
            "balance": data.get('balance', 0),
            "diamonds": data.get('diamonds', 0),
            "referrals": data.get('referrals', 0),
            "last_active": today,
            "json_data": data # Ğ’ĞµÑÑŒ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ° ĞºĞ»Ğ°Ğ´ĞµĞ¼ Ğ² ĞºĞ¾Ğ»Ğ¾Ğ½ĞºÑƒ json_data
        }
        data_list.append(row)

    # Ğ Ğ°Ğ·Ğ±Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° Ğ¿Ğ°Ñ‡ĞºĞ¸ Ğ¿Ğ¾ 100 ÑˆÑ‚ÑƒĞº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ñ‚ÑŒ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°
    chunk_size = 100
    url = f"{SUPABASE_URL}/rest/v1/users"
    
    # Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ´Ğ»Ñ UPSERT (ÑĞ»Ğ¸ÑĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ID)
    upsert_headers = HEADERS.copy()
    upsert_headers["Prefer"] = "resolution=merge-duplicates"

    async with aiohttp.ClientSession() as session:
        for i in range(0, len(data_list), chunk_size):
            chunk = data_list[i:i + chunk_size]
            try:
                async with session.post(url, headers=upsert_headers, json=chunk) as resp:
                    if resp.status not in [200, 201, 204]:
                        text = await resp.text()
                        logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Supabase: {resp.status} - {text}")
            except Exception as e:
                logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº Supabase: {e}")

async def load_all_users():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Supabase Ñ‡ĞµÑ€ĞµĞ· GET Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ."""
    loaded_users = {}
    url = f"{SUPABASE_URL}/rest/v1/users?select=user_id,json_data"
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=HEADERS) as resp:
                if resp.status == 200:
                    rows = await resp.json()
                    for row in rows:
                        user_id = row['user_id']
                        user_data = row['json_data']
                        
                        # Ğ•ÑĞ»Ğ¸ Ğ²Ğ´Ñ€ÑƒĞ³ Ğ¿Ñ€Ğ¸ÑˆĞ»Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹ (Ğ±Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ²ĞµÑ€ÑĞ¸ÑÑ…)
                        if isinstance(user_data, str):
                            user_data = json.loads(user_data)
                            
                        loaded_users[int(user_id)] = user_data
                    
                    logging.warning(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(loaded_users)} Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Supabase.")
                else:
                    text = await resp.text()
                    logging.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ· Supabase: {resp.status} - {text}")
                    
    except Exception as e:
        logging.error(f"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        
    return loaded_users

async def delete_inactive_users(days=90):
    """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."""
    cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
    
    # Ğ¡Ğ¸Ğ½Ñ‚Ğ°ĞºÑĞ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Supabase: last_active=lt.DATE (lt = less than / Ğ¼ĞµĞ½ÑŒÑˆĞµ Ñ‡ĞµĞ¼)
    url = f"{SUPABASE_URL}/rest/v1/users?last_active=lt.{cutoff_date}"
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.delete(url, headers=HEADERS) as resp:
                if resp.status == 204:
                    logging.warning(f"ğŸ§¹ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ°.")
                else:
                    logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸: {resp.status}")
    except Exception as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸: {e}")

async def export_users_to_json_file():
    """
    Ğ’Ñ‹Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ±Ğ°Ğ·Ñƒ Ğ² Ñ„Ğ°Ğ¹Ğ» (ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑÑ‘ Ğ¸Ğ· Supabase).
    """
    url = f"{SUPABASE_URL}/rest/v1/users?select=json_data"
    filename = "users_export.json"
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=HEADERS) as resp:
                if resp.status == 200:
                    rows = await resp.json()
                    all_data = [row['json_data'] for row in rows]
                    
                    with open(filename, "w", encoding="utf-8") as f:
                        json.dump(all_data, f, ensure_ascii=False, indent=4)
                    return filename
                else:
                    logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°: {resp.status}")
    except Exception as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ°: {e}")
    
    return None