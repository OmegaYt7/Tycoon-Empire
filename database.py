import aiohttp
import json
import logging
import asyncio
from datetime import datetime, timedelta

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ SUPABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUPABASE_URL = "https://tuvqserdclbgloysblrx.supabase.co"
SUPABASE_KEY = "sb_secret_bDIUtmYZ2Zx5Rz3EauEhlw_sbrmR6y9" 

http_session = None

HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=minimal",
    "Connection": "keep-alive"
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ˜ Ğ‘ĞĞ—Ğ« Ğ”ĞĞĞĞ«Ğ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def create_pool():
    global http_session
    if http_session is None:
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        http_session = aiohttp.ClientSession(headers=HEADERS, timeout=timeout)
        logging.warning("âœ… Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¸ Supabase...")

async def close_session():
    global http_session
    if http_session:
        await http_session.close()
        logging.warning("ğŸ”Œ Ğ¡ĞµÑÑĞ¸Ñ Supabase Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°.")

async def create_table():
    pass

# --- ĞĞĞ’ĞĞ¯ Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯ Ğ”Ğ›Ğ¯ Ğ¡ĞĞ¥Ğ ĞĞĞ•ĞĞ˜Ğ¯ ĞĞ”ĞĞĞ“Ğ Ğ˜Ğ“Ğ ĞĞšĞ ---
async def save_user(user_id, user_data):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ğ´Ğ»Ñ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞµĞ¹Ğ²Ğ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ°Ñ…)."""
    global http_session
    if http_session is None: await create_pool()
    
    url = f"{SUPABASE_URL}/rest/v1/users"
    headers = {"Prefer": "resolution=merge-duplicates"}
    
    row = {
        "user_id": user_id,
        "username": user_data.get('username', 'Guest'),
        "nickname": user_data.get('nickname', 'Unknown'),
        "balance": user_data.get('balance', 0),
        "diamonds": user_data.get('diamonds', 0),
        "referrals": user_data.get('referrals', 0),
        "last_active": datetime.now().strftime("%Y-%m-%d"),
        "json_data": user_data
    }
    
    try:
        async with http_session.post(url, headers=headers, json=[row]) as resp:
            if resp.status not in [200, 201, 204]:
                logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ° {user_id}: {resp.status}")
    except Exception as e:
        logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ³Ñ€Ğ¾ĞºĞ° {user_id}: {e}")

async def save_all_users(users_dict):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹."""
    if not users_dict: return

    global http_session
    if http_session is None: await create_pool()

    today = datetime.now().strftime("%Y-%m-%d")
    data_list = []
    
    for user_id, data in users_dict.items():
        row = {
            "user_id": user_id,
            "username": data.get('username', 'Guest'),
            "nickname": data.get('nickname', 'Unknown'),
            "balance": data.get('balance', 0),
            "diamonds": data.get('diamonds', 0),
            "referrals": data.get('referrals', 0),
            "last_active": today,
            "json_data": data
        }
        data_list.append(row)

    chunk_size = 50 # Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ» Ñ‡Ğ°Ğ½Ğº Ğ´Ğ»Ñ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸
    url = f"{SUPABASE_URL}/rest/v1/users"
    headers = {"Prefer": "resolution=merge-duplicates"} 
    
    for i in range(0, len(data_list), chunk_size):
        chunk = data_list[i:i + chunk_size]
        try:
            async with http_session.post(url, headers=headers, json=chunk) as resp:
                if resp.status not in [200, 201, 204]:
                    logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¼Ğ°ÑÑĞ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ: {resp.status}")
        except Exception as e:
            logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğº Supabase: {e}")

async def load_all_users():
    global http_session
    if http_session is None: return {}

    loaded_users = {}
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ’Ğ¡Ğ• Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ, Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ° Ğ½ĞµÑ‚, Ğ½Ğ¾ Supabase Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ñ‚Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 1000 ÑÑ‚Ñ€Ğ¾Ğº Ğ·Ğ° Ñ€Ğ°Ğ·
    # Ğ”Ğ»Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ñ…Ğ²Ğ°Ñ‚Ğ¸Ñ‚, Ğ¿Ñ€Ğ¸ Ñ€Ğ¾ÑÑ‚Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ Ğ¿Ğ°Ğ³Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ
    url = f"{SUPABASE_URL}/rest/v1/users?select=user_id,json_data"
    
    try:
        async with http_session.get(url) as resp:
            if resp.status == 200:
                rows = await resp.json()
                for row in rows:
                    user_id = row['user_id']
                    user_data = row['json_data']
                    if isinstance(user_data, str):
                        try: user_data = json.loads(user_data)
                        except: continue
                    loaded_users[int(user_id)] = user_data
                logging.warning(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(loaded_users)} Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.")
            else:
                logging.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {resp.status}")
    except Exception as e:
        logging.error(f"ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        
    return loaded_users

async def export_users_to_json_file():
    global http_session
    if http_session is None: return None
    url = f"{SUPABASE_URL}/rest/v1/users?select=json_data"
    filename = "users_export.json"
    try:
        async with http_session.get(url) as resp:
            if resp.status == 200:
                rows = await resp.json()
                all_data = [row['json_data'] for row in rows]
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=4)
                return filename
    except Exception:
        pass
    return None
