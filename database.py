import aiohttp
import json
import logging
import asyncio
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯ SUPABASE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUPABASE_URL = "https://tuvqserdclbgloysblrx.supabase.co"
SUPABASE_KEY = "sb_secret_bDIUtmYZ2Zx5Rz3EauEhlw_sbrmR6y9" 

http_session = None

# Ğ£Ğ±Ñ€Ğ°Ğ»Ğ¸ "Connection": "keep-alive", Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ aiohttp ÑĞ°Ğ¼ Ñ€ĞµÑˆĞ°Ğ» Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¿ĞµÑ€ĞµĞ¿Ğ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ
HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json",
    "Prefer": "return=minimal"
}

async def create_pool():
    """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞµÑÑĞ¸Ñ aiohttp Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ API Supabase."""
    global http_session
    if http_session is None or http_session.closed:
        # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ»Ğ¸ Ñ‚Ğ°Ğ¹Ğ¼-Ğ°ÑƒÑ‚Ñ‹ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        timeout = aiohttp.ClientTimeout(total=45, connect=15, sock_connect=15)
        http_session = aiohttp.ClientSession(headers=HEADERS, timeout=timeout)
        logging.warning("âœ… Ğ¡ĞµÑÑĞ¸Ñ Supabase Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°.")

async def close_session():
    """Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞµÑÑĞ¸Ñ aiohttp Ğ¿Ñ€Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞµ Ğ±Ğ¾Ñ‚Ğ°."""
    global http_session
    if http_session and not http_session.closed:
        await http_session.close()
        logging.warning("ğŸ”Œ Ğ¡ĞµÑÑĞ¸Ñ Supabase Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ°.")

async def save_user(user_id, user_data):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ Ğ² Ğ±Ğ°Ğ·Ñƒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…."""
    global http_session
    if http_session is None or http_session.closed: await create_pool()
    
    url = f"{SUPABASE_URL}/rest/v1/users"
    headers = {"Prefer": "resolution=merge-duplicates"}
    
    row = {
        "user_id": user_id,
        "username": user_data.get('username', 'Guest'),
        "nickname": user_data.get('nickname', 'Unknown'),
        "balance": user_data.get('balance', 0),
        "diamonds": user_data.get('diamonds', 0),
        "referrals": user_data.get('referrals', 0),
        "last_active": datetime.now().strftime("%Y-%m-%d"),
        "json_data": user_data
    }
    
    try:
        async with http_session.post(url, headers=headers, json=[row]) as resp:
            if resp.status not in [200, 201, 204]:
                logging.error(f"Save User Error {user_id}: {resp.status}")
    except Exception as e:
        logging.error(f"Save User Exception {user_id}: {e}")

async def save_all_users(users_dict):
    """ĞœĞ°ÑÑĞ¾Ğ²Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº."""
    if not users_dict: return

    global http_session
    if http_session is None or http_session.closed: await create_pool()

    today = datetime.now().strftime("%Y-%m-%d")
    data_list = []
    
    for user_id, data in users_dict.items():
        row = {
            "user_id": user_id,
            "username": data.get('username', 'Guest'),
            "nickname": data.get('nickname', 'Unknown'),
            "balance": data.get('balance', 0),
            "diamonds": data.get('diamonds', 0),
            "referrals": data.get('referrals', 0),
            "last_active": today,
            "json_data": data
        }
        data_list.append(row)

    chunk_size = 50 
    url = f"{SUPABASE_URL}/rest/v1/users"
    headers = {"Prefer": "resolution=merge-duplicates"} 
    
    for i in range(0, len(data_list), chunk_size):
        chunk = data_list[i:i + chunk_size]
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ 3 Ñ€Ğ°Ğ·Ğ°, ĞµÑĞ»Ğ¸ ÑĞµÑ‚ÑŒ Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ°
        for attempt in range(3):
            try:
                async with http_session.post(url, headers=headers, json=chunk) as resp:
                    if resp.status in [200, 201, 204]:
                        break 
                    logging.error(f"Bulk Save Error: {resp.status}")
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                if attempt == 2:
                    logging.error(f"Bulk Save Final Failure: {e}")
                else:
                    await asyncio.sleep(1) # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ¼
        await asyncio.sleep(0.3) # ĞŸĞ°ÑƒĞ·Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ñ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ² API

async def load_all_users():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Supabase."""
    global http_session
    if http_session is None or http_session.closed: await create_pool()

    loaded_users = {}
    url = f"{SUPABASE_URL}/rest/v1/users?select=user_id,json_data"
    
    try:
        async with http_session.get(url) as resp:
            if resp.status == 200:
                rows = await resp.json()
                for row in rows:
                    user_id = row['user_id']
                    user_data = row['json_data']
                    if isinstance(user_data, str):
                        try: user_data = json.loads(user_data)
                        except: continue
                    loaded_users[int(user_id)] = user_data
                logging.warning(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(loaded_users)} Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹.")
            else:
                logging.error(f"Load Error: {resp.status}")
    except Exception as e:
        logging.error(f"Load Exception: {e}")
        
    return loaded_users

async def export_users_to_json_file():
    """Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ²ÑĞµÑ… Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹ Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ JSON Ñ„Ğ°Ğ¹Ğ»."""
    global http_session
    if http_session is None or http_session.closed: await create_pool()
    url = f"{SUPABASE_URL}/rest/v1/users?select=json_data"
    filename = "users_export.json"
    try:
        async with http_session.get(url) as resp:
            if resp.status == 200:
                rows = await resp.json()
                all_data = [row['json_data'] for row in rows]
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, ensure_ascii=False, indent=4)
                return filename
    except Exception:
        pass
    return None
